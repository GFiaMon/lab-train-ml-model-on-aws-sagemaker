{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cee634f-d34e-4636-a140-d4660b3cb7cf",
   "metadata": {},
   "source": [
    "# Lab Sagemaker Deployment\n",
    "## Cell 1: Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e1b9c5d-c264-4364-b51a-6e81bf5bea20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "‚úÖ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import Session\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "from sagemaker.session import TrainingInput\n",
    "import boto3\n",
    "import os\n",
    "import shap\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004a95dc-193c-47ec-989a-4274f23a2473",
   "metadata": {},
   "source": [
    "## Cell 2: Initialize SageMaker Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea34f01f-38af-4f0e-b60f-a5a34422ead6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß SageMaker Initialization Complete:\n",
      "   AWS Region: eu-central-1\n",
      "   S3 Bucket: sagemaker-eu-central-1-238708039523\n",
      "   Role: AmazonSageMakerServiceCatalogProductsUseRole\n"
     ]
    }
   ],
   "source": [
    "def initialize_sagemaker():\n",
    "    \"\"\"Initialize SageMaker session and get basic info\"\"\"\n",
    "    session = Session()\n",
    "    bucket = session.default_bucket()\n",
    "    role = sagemaker.get_execution_role()\n",
    "    region = session.boto_region_name\n",
    "    \n",
    "    print(\"üîß SageMaker Initialization Complete:\")\n",
    "    print(f\"   AWS Region: {region}\")\n",
    "    print(f\"   S3 Bucket: {bucket}\")\n",
    "    print(f\"   Role: {role.split('/')[-1]}\")\n",
    "    \n",
    "    return session, bucket, role, region\n",
    "\n",
    "# Run initialization\n",
    "session, bucket, role, region = initialize_sagemaker()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a056bb5a-3aa4-4be2-9900-399c4bd00fa1",
   "metadata": {},
   "source": [
    "## 3: Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b223b634-0765-4987-aa66-971ff96768b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading iris dataset...\n",
      "‚úÖ Data loaded successfully!\n",
      "   Training set: (120, 5)\n",
      "   Test set: (30, 5)\n",
      "   Classes: [0 1 2]\n",
      "\n",
      "First 5 rows of training data:\n",
      "     target  sepal_length  sepal_width  petal_length  petal_width\n",
      "8         0           4.4          2.9           1.4          0.2\n",
      "106       2           4.9          2.5           4.5          1.7\n",
      "76        1           6.8          2.8           4.8          1.4\n",
      "9         0           4.9          3.1           1.5          0.1\n",
      "89        1           5.5          2.5           4.0          1.3\n"
     ]
    }
   ],
   "source": [
    "def load_and_prepare_iris_data():\n",
    "    \"\"\"Load and prepare iris dataset for SageMaker\"\"\"\n",
    "    print(\"üìä Loading iris dataset...\")\n",
    "    \n",
    "    # Load iris dataset\n",
    "    iris = load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "    \n",
    "    # Convert to DataFrame for better handling\n",
    "    feature_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "    X_df = pd.DataFrame(X, columns=feature_names)\n",
    "    y_series = pd.Series(y, name='target')\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_df, y_series, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Create DataFrames with target variable (SageMaker expects target in first column)\n",
    "    train_data = pd.concat([y_train, X_train], axis=1)\n",
    "    test_data = pd.concat([y_test, X_test], axis=1)\n",
    "    \n",
    "    print(f\"‚úÖ Data loaded successfully!\")\n",
    "    print(f\"   Training set: {train_data.shape}\")\n",
    "    print(f\"   Test set: {test_data.shape}\")\n",
    "    print(f\"   Classes: {np.unique(y)}\")\n",
    "    \n",
    "    return train_data, test_data, X_train, X_test, y_train, y_test\n",
    "\n",
    "# Test the function\n",
    "train_data, test_data, X_train, X_test, y_train, y_test = load_and_prepare_iris_data()\n",
    "print(\"\\nFirst 5 rows of training data:\")\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b3731f-2deb-434d-b5ff-bae40f1ef18e",
   "metadata": {},
   "source": [
    "## 4: Save Data Locally Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dc35361-0423-48fc-8cae-4fc36d1bf1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving data locally...\n",
      "‚úÖ Data saved locally!\n",
      "   train.csv (for SageMaker)\n",
      "   train_with_headers.csv (for inspection)\n",
      "   test.csv (for SageMaker)\n",
      "   test_with_headers.csv (for inspection)\n",
      "\n",
      "üìÅ Local files created:\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  540 Oct 27 16:22 test.csv\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  597 Oct 27 16:22 test_with_headers.csv\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 2160 Oct 27 16:22 train.csv\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 2217 Oct 27 16:22 train_with_headers.csv\n"
     ]
    }
   ],
   "source": [
    "def save_data_locally(train_data, test_data):\n",
    "    \"\"\"Save train and test data to local files\"\"\"\n",
    "    print(\"üíæ Saving data locally...\")\n",
    "    \n",
    "    # Save without headers for SageMaker\n",
    "    train_data.to_csv('train.csv', index=False, header=False)\n",
    "    test_data.to_csv('test.csv', index=False, header=False)\n",
    "    \n",
    "    # Also save with headers for inspection\n",
    "    train_data.to_csv('train_with_headers.csv', index=False)\n",
    "    test_data.to_csv('test_with_headers.csv', index=False)\n",
    "    \n",
    "    print(\"‚úÖ Data saved locally!\")\n",
    "    print(\"   train.csv (for SageMaker)\")\n",
    "    print(\"   train_with_headers.csv (for inspection)\")\n",
    "    print(\"   test.csv (for SageMaker)\") \n",
    "    print(\"   test_with_headers.csv (for inspection)\")\n",
    "\n",
    "# Run the function\n",
    "save_data_locally(train_data, test_data)\n",
    "\n",
    "# Verify the files\n",
    "print(\"\\nüìÅ Local files created:\")\n",
    "!ls -la *.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1a8e48-657d-4e8b-a2d3-aa0d5c4e1257",
   "metadata": {},
   "source": [
    "## 5: Upload to S3 Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaabb767-ec2c-4e2d-90a4-6af1242b2eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚òÅÔ∏è Uploading data to S3...\n",
      "‚úÖ Data uploaded to S3!\n",
      "   s3://sagemaker-eu-central-1-238708039523/iris-classification-demo/data/train.csv\n",
      "   s3://sagemaker-eu-central-1-238708039523/iris-classification-demo/data/test.csv\n",
      "\n",
      "üìã Verifying S3 upload:\n",
      "2025-10-27 16:22:18        540 iris-classification-demo/data/test.csv\n",
      "2025-10-27 16:22:18       2160 iris-classification-demo/data/train.csv\n"
     ]
    }
   ],
   "source": [
    "def upload_to_s3(bucket, prefix):\n",
    "    \"\"\"Upload data to S3 bucket\"\"\"\n",
    "    print(\"‚òÅÔ∏è Uploading data to S3...\")\n",
    "    \n",
    "    s3 = boto3.Session().resource('s3')\n",
    "    \n",
    "    # Upload training data\n",
    "    s3.Bucket(bucket).Object(\n",
    "        f\"{prefix}/data/train.csv\").upload_file('train.csv')\n",
    "    \n",
    "    # Upload test data  \n",
    "    s3.Bucket(bucket).Object(\n",
    "        f\"{prefix}/data/test.csv\").upload_file('test.csv')\n",
    "    \n",
    "    print(f\"‚úÖ Data uploaded to S3!\")\n",
    "    print(f\"   s3://{bucket}/{prefix}/data/train.csv\")\n",
    "    print(f\"   s3://{bucket}/{prefix}/data/test.csv\")\n",
    "    \n",
    "    # Verify upload\n",
    "    print(\"\\nüìã Verifying S3 upload:\")\n",
    "    !aws s3 ls {bucket}/{prefix}/data/ --recursive\n",
    "\n",
    "# Run the function\n",
    "prefix = \"iris-classification-demo\"\n",
    "upload_to_s3(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ad4b66-a75c-4562-8bc0-09f934e035a1",
   "metadata": {},
   "source": [
    "## 6: Create and Train Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cb8a387-5c13-4776-a1ae-24b4a8905fca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2025-10-27-16-29-26-917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Starting XGBoost training...\n",
      "ü§ñ Creating and training XGBoost model...\n",
      "   Using container: 492215442770.dkr.ecr.eu-central-1.amazonaws.com/sagemaker-xgboost:1.2-1\n",
      "   Starting XGBoost training...\n",
      "2025-10-27 16:29:28 Starting - Starting the training job...\n",
      "2025-10-27 16:29:52 Starting - Preparing the instances for training...\n",
      "2025-10-27 16:30:24 Downloading - Downloading input data...\n",
      "2025-10-27 16:30:54 Downloading - Downloading the training image.........\n",
      "2025-10-27 16:32:26 Training - Training image download completed. Training in progress.\n",
      "2025-10-27 16:32:26 Uploading - Uploading generated training model\u001b[34m[2025-10-27 16:32:17.484 ip-10-0-78-53.eu-central-1.compute.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value multi:softmax to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34m[2025-10-27 16:32:17.572 ip-10-0-78-53.eu-central-1.compute.internal:7 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2025-10-27 16:32:17.573 ip-10-0-78-53.eu-central-1.compute.internal:7 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2025-10-27 16:32:17.573 ip-10-0-78-53.eu-central-1.compute.internal:7 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2025-10-27 16:32:17.574 ip-10-0-78-53.eu-central-1.compute.internal:7 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2025-10-27 16:32:17.574 ip-10-0-78-53.eu-central-1.compute.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mINFO:root:Debug hook created from config\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 120 rows and 4 columns\u001b[0m\n",
      "\u001b[34mINFO:root:Validation matrix has 30 rows\u001b[0m\n",
      "\u001b[34m[0]#011train-merror:0.01667#011validation-merror:0.03333\u001b[0m\n",
      "\u001b[34m[2025-10-27 16:32:17.577 ip-10-0-78-53.eu-central-1.compute.internal:7 INFO hook.py:413] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2025-10-27 16:32:17.580 ip-10-0-78-53.eu-central-1.compute.internal:7 INFO hook.py:476] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[1]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[2]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[3]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[4]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[5]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[6]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[7]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[8]#011train-merror:0.01667#011validation-merror:0.03333\u001b[0m\n",
      "\u001b[34m[9]#011train-merror:0.01667#011validation-merror:0.03333\u001b[0m\n",
      "\u001b[34m[10]#011train-merror:0.01667#011validation-merror:0.03333\u001b[0m\n",
      "\u001b[34m[11]#011train-merror:0.01667#011validation-merror:0.03333\u001b[0m\n",
      "\u001b[34m[12]#011train-merror:0.01667#011validation-merror:0.03333\u001b[0m\n",
      "\u001b[34m[13]#011train-merror:0.01667#011validation-merror:0.03333\u001b[0m\n",
      "\u001b[34m[14]#011train-merror:0.01667#011validation-merror:0.03333\u001b[0m\n",
      "\u001b[34m[15]#011train-merror:0.01667#011validation-merror:0.03333\u001b[0m\n",
      "\u001b[34m[16]#011train-merror:0.01667#011validation-merror:0.03333\u001b[0m\n",
      "\u001b[34m[17]#011train-merror:0.01667#011validation-merror:0.03333\u001b[0m\n",
      "\u001b[34m[18]#011train-merror:0.01667#011validation-merror:0.03333\u001b[0m\n",
      "\u001b[34m[19]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[20]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[21]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[22]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[23]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[24]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[25]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[26]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[27]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[28]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[29]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[30]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[31]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[32]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[33]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[34]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[35]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[36]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[37]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[38]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[39]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[40]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[41]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[42]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[43]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[44]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[45]#011train-merror:0.01667#011validation-merror:0.03333\u001b[0m\n",
      "\u001b[34m[46]#011train-merror:0.01667#011validation-merror:0.03333\u001b[0m\n",
      "\u001b[34m[47]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[48]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\u001b[34m[49]#011train-merror:0.03333#011validation-merror:0.06667\u001b[0m\n",
      "\n",
      "2025-10-27 16:32:39 Completed - Training job completed\n",
      "Training seconds: 134\n",
      "Billable seconds: 134\n",
      "‚úÖ XGBoost training completed!\n"
     ]
    }
   ],
   "source": [
    "def create_and_train_xgboost(bucket, prefix, role):\n",
    "    \"\"\"Create and train XGBoost model (reliable fallback)\"\"\"\n",
    "    print(\"ü§ñ Creating and training XGBoost model...\")\n",
    "    \n",
    "    region = Session().boto_region_name\n",
    "    container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-1\")\n",
    "    print(f\"   Using container: {container}\")\n",
    "    \n",
    "    s3_output_location = f's3://{bucket}/{prefix}/xgboost_model'\n",
    "    \n",
    "    xgb_model = sagemaker.estimator.Estimator(\n",
    "        image_uri=container,\n",
    "        role=role,\n",
    "        instance_count=1,\n",
    "        instance_type='ml.m4.xlarge',\n",
    "        volume_size=5,\n",
    "        output_path=s3_output_location,\n",
    "        sagemaker_session=Session()\n",
    "    )\n",
    "    \n",
    "    # Set hyperparameters for multi-class classification\n",
    "    xgb_model.set_hyperparameters(\n",
    "        max_depth=5,\n",
    "        eta=0.2,\n",
    "        gamma=4,\n",
    "        min_child_weight=1,\n",
    "        subsample=0.8,\n",
    "        objective=\"multi:softmax\",\n",
    "        num_class=3,\n",
    "        num_round=50\n",
    "    )\n",
    "    \n",
    "    from sagemaker.inputs import TrainingInput\n",
    "    \n",
    "    train_input = TrainingInput(\n",
    "        f\"s3://{bucket}/{prefix}/data/train.csv\", content_type=\"csv\"\n",
    "    )\n",
    "    \n",
    "    test_input = TrainingInput(\n",
    "        f\"s3://{bucket}/{prefix}/data/test.csv\", content_type=\"csv\"\n",
    "    )\n",
    "    \n",
    "    print(\"   Starting XGBoost training...\")\n",
    "    xgb_model.fit({\"train\": train_input, \"validation\": test_input}, wait=True)\n",
    "    \n",
    "    print(\"‚úÖ XGBoost training completed!\")\n",
    "    return xgb_model\n",
    "\n",
    "# Run XGBoost instead\n",
    "print(\"üéØ Starting XGBoost training...\")\n",
    "trained_model = create_and_train_xgboost(bucket, prefix, role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91266a9d-c6f6-4444-af87-8837f424203b",
   "metadata": {},
   "source": [
    "## 6: Deploy Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b36c758-70ee-4955-851f-0db58b312e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2025-10-27-16-33-25-041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Deploying model to endpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating endpoint-config with name iris-classifier-2025-10-27-16-33-25\n",
      "INFO:sagemaker:Creating endpoint with name iris-classifier-2025-10-27-16-33-25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!‚úÖ Model deployed successfully!\n",
      "   Endpoint name: iris-classifier-2025-10-27-16-33-25\n",
      "   Instance type: ml.t2.medium\n"
     ]
    }
   ],
   "source": [
    "def deploy_model(model):\n",
    "    \"\"\"Deploy the trained model to an endpoint\"\"\"\n",
    "    print(\"üöÄ Deploying model to endpoint...\")\n",
    "    \n",
    "    predictor = model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type='ml.t2.medium',\n",
    "        serializer=CSVSerializer(),\n",
    "        deserializer=CSVDeserializer(),\n",
    "        endpoint_name=f\"iris-classifier-{pd.Timestamp.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Model deployed successfully!\")\n",
    "    print(f\"   Endpoint name: {predictor.endpoint_name}\")\n",
    "    print(f\"   Instance type: ml.t2.medium\")\n",
    "    \n",
    "    return predictor\n",
    "\n",
    "# Run the function\n",
    "predictor = deploy_model(trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ced7bab-e6ab-451b-ba66-fa2bb4cc512d",
   "metadata": {},
   "source": [
    "## 7: Make Predictions Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "672856ff-7126-4a87-a1fc-5fb8394a68fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÆ Making predictions...\n",
      "   Sample data shape: (10, 4)\n",
      "\n",
      "üìä Prediction Results:\n",
      "==================================================\n",
      "Sample 1: True=0, Predicted=0 ‚úÖ\n",
      "\n",
      "üìà Sample Accuracy: 40.0%\n",
      "\n",
      "üîç Sample test data used for predictions:\n",
      "     sepal_length  sepal_width  petal_length  petal_width\n",
      "38            4.4          3.0           1.3          0.2\n",
      "127           6.1          3.0           4.9          1.8\n",
      "57            4.9          2.4           3.3          1.0\n",
      "93            5.0          2.3           3.3          1.0\n",
      "42            4.4          3.2           1.3          0.2\n",
      "56            6.3          3.3           4.7          1.6\n",
      "22            4.6          3.6           1.0          0.2\n",
      "20            5.4          3.4           1.7          0.2\n",
      "147           6.5          3.0           5.2          2.0\n",
      "84            5.4          3.0           4.5          1.5\n"
     ]
    }
   ],
   "source": [
    "def make_predictions(predictor, X_test, y_test, sample_size=10):\n",
    "    \"\"\"Make predictions using the deployed model\"\"\"\n",
    "    print(\"üîÆ Making predictions...\")\n",
    "    \n",
    "    # Take a sample for prediction\n",
    "    sample_data = X_test.iloc[:sample_size]\n",
    "    true_labels = y_test.iloc[:sample_size]\n",
    "    \n",
    "    print(f\"   Sample data shape: {sample_data.shape}\")\n",
    "    \n",
    "    # Convert to CSV format and make predictions\n",
    "    predictions = predictor.predict(sample_data.values)\n",
    "    \n",
    "    # Convert predictions to integers (class labels)\n",
    "    predicted_labels = [int(float(pred[0])) for pred in predictions]\n",
    "    \n",
    "    print(\"\\nüìä Prediction Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    for i, (true_val, pred_val) in enumerate(zip(true_labels, predicted_labels)):\n",
    "        status = \"‚úÖ\" if true_val == pred_val else \"‚ùå\"\n",
    "        print(f\"Sample {i+1}: True={true_val}, Predicted={pred_val} {status}\")\n",
    "    \n",
    "    # Calculate accuracy on the sample\n",
    "    accuracy = np.sum(np.array(true_labels) == np.array(predicted_labels)) / len(true_labels)\n",
    "    print(f\"\\nüìà Sample Accuracy: {accuracy:.1%}\")\n",
    "    \n",
    "    return predictions, predicted_labels\n",
    "\n",
    "# Run the function\n",
    "predictions, predicted_labels = make_predictions(predictor, X_test, y_test)\n",
    "\n",
    "# Show some test samples\n",
    "print(\"\\nüîç Sample test data used for predictions:\")\n",
    "print(X_test.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaa1a72-dd2d-4567-8557-c18840fb133a",
   "metadata": {},
   "source": [
    "## 8: Test with Custom Data Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "424b33bf-fd78-41ad-8042-3ed3b266d9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing with custom data...\n",
      "\n",
      "üéØ Custom Test Results:\n",
      "============================================================\n",
      "Test 1: Features=[5.1, 3.5, 1.4, 0.2] -> setosa (class 0)\n"
     ]
    }
   ],
   "source": [
    "def test_custom_predictions(predictor):\n",
    "    \"\"\"Test the model with custom input data\"\"\"\n",
    "    print(\"üß™ Testing with custom data...\")\n",
    "    \n",
    "    # Create some custom test cases based on iris dataset characteristics\n",
    "    custom_test_cases = [\n",
    "        [5.1, 3.5, 1.4, 0.2],  # Should predict class 0 (setosa)\n",
    "        [6.7, 3.0, 5.2, 2.3],  # Should predict class 2 (virginica)\n",
    "        [5.9, 3.0, 4.2, 1.5],  # Should predict class 1 (versicolor)\n",
    "        [4.9, 3.0, 1.4, 0.2],  # Should predict class 0 (setosa)\n",
    "        [6.3, 3.3, 6.0, 2.5]   # Should predict class 2 (virginica)\n",
    "    ]\n",
    "    \n",
    "    class_names = ['setosa', 'versicolor', 'virginica']\n",
    "    \n",
    "    predictions = predictor.predict(custom_test_cases)\n",
    "    predicted_classes = [int(float(pred[0])) for pred in predictions]\n",
    "    \n",
    "    print(\"\\nüéØ Custom Test Results:\")\n",
    "    print(\"=\" * 60)\n",
    "    for i, (features, pred_class) in enumerate(zip(custom_test_cases, predicted_classes)):\n",
    "        print(f\"Test {i+1}: Features={features} -> {class_names[pred_class]} (class {pred_class})\")\n",
    "    \n",
    "    return predicted_classes\n",
    "\n",
    "# Run the function\n",
    "custom_predictions = test_custom_predictions(predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6832400c-d761-44ba-b021-5633b42b919d",
   "metadata": {},
   "source": [
    "## 9: Cleanup Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ebe567c-6807-4f0d-8b17-e5e489ae8333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° Note: Run cleanup_resources(predictor) when done to avoid charges\n"
     ]
    }
   ],
   "source": [
    "def cleanup_resources(predictor):\n",
    "    \"\"\"Clean up the endpoint to avoid ongoing charges\"\"\"\n",
    "    print(\"üßπ Cleaning up resources...\")\n",
    "    \n",
    "    try:\n",
    "        predictor.delete_endpoint()\n",
    "        predictor.delete_model()\n",
    "        print(\"‚úÖ Endpoint and model deleted successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Cleanup warning: {e}\")\n",
    "    \n",
    "    # Also clean local files\n",
    "    try:\n",
    "        os.remove('train.csv')\n",
    "        os.remove('test.csv') \n",
    "        os.remove('train_with_headers.csv')\n",
    "        os.remove('test_with_headers.csv')\n",
    "        print(\"‚úÖ Local files cleaned up!\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Could not clean all local files\")\n",
    "\n",
    "# Uncomment the line below when you want to clean up\n",
    "# cleanup_resources(predictor)\n",
    "print(\"üí° Note: Run cleanup_resources(predictor) when done to avoid charges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2897b9-bc4c-4d48-85be-c5837d8b741a",
   "metadata": {},
   "source": [
    "## 10: Complete Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1ffb15-f862-43bb-975f-eb1d134ab442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_pipeline():\n",
    "    \"\"\"Run the complete pipeline from start to finish\"\"\"\n",
    "    print(\"üéØ Starting Complete Iris Classification Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load data\n",
    "        train_data, test_data, X_train, X_test, y_train, y_test = load_and_prepare_iris_data()\n",
    "        \n",
    "        # Step 2: Save locally\n",
    "        save_data_locally(train_data, test_data)\n",
    "        \n",
    "        # Step 3: Upload to S3\n",
    "        upload_to_s3(bucket, prefix)\n",
    "        \n",
    "        # Step 4: Train model\n",
    "        trained_model = create_and_train_model(bucket, prefix, role)\n",
    "        \n",
    "        # Step 5: Deploy model\n",
    "        predictor = deploy_model(trained_model)\n",
    "        \n",
    "        # Step 6: Make predictions\n",
    "        predictions, pred_labels = make_predictions(predictor, X_test, y_test)\n",
    "        \n",
    "        # Step 7: Test custom data\n",
    "        test_custom_predictions(predictor)\n",
    "        \n",
    "        print(\"\\nüéâ Pipeline completed successfully!\")\n",
    "        return predictor\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Pipeline failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Uncomment to run the complete pipeline\n",
    "# final_predictor = run_complete_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bec2f93-8b21-40e8-868c-3552fab616b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8e8b86-23f7-4f24-867d-0accb4cd156f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844ddb53-e562-4555-bcf9-dcd1b7316260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
